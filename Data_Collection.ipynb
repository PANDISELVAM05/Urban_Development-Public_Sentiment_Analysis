{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796f9980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 311 Service Requests generated with complaint-related categories\n",
      "✅ All datasets saved independently & compressed into: city_datasets\\city_datasets_10000_realistic.zip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os\n",
    "from faker import Faker\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime, timedelta\n",
    "import zipfile\n",
    "\n",
    "# ------------------------------\n",
    "# Initialize Faker and output directory\n",
    "# ------------------------------\n",
    "fake = Faker()\n",
    "output_dir = \"city_datasets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "n = 10000  # Minimum records per dataset\n",
    "\n",
    "# ------------------------------\n",
    "# Expanded Tamil Nadu locations and stops\n",
    "# ------------------------------\n",
    "tn_locations = [\n",
    "    \"Chennai\", \"Coimbatore\", \"Madurai\", \"Tiruchirappalli\", \"Tirunelveli\",\n",
    "    \"Salem\", \"Erode\", \"Vellore\", \"Thoothukudi\", \"Dindigul\",\n",
    "    \"Kanchipuram\", \"Hosur\", \"Cuddalore\", \"Karur\", \"Nagapattinam\",\n",
    "    \"Sivakasi\", \"Tiruvannamalai\", \"Krishnagiri\", \"Namakkal\", \"Virudhunagar\",\n",
    "    \"Perambalur\", \"Ariyalur\", \"Dharmapuri\", \"Tiruvarur\", \"Ramanathapuram\"\n",
    "]\n",
    "\n",
    "tn_stops = [\n",
    "    \"Chennai Central\", \"Egmore\", \"Tambaram\", \"Koyambedu\", \"Vadapalani\",\n",
    "    \"Coimbatore Junction\", \"Salem Town\", \"Madurai Junction\", \"Trichy Fort\",\n",
    "    \"Tirunelveli\", \"Nagercoil\", \"Vellore Town\", \"Erode Junction\", \"Thanjavur\",\n",
    "    \"Kanchipuram Stop\", \"Hosur Stop\", \"Cuddalore Junction\", \"Karur Stop\",\n",
    "    \"Nagapattinam Stop\", \"Sivakasi Stop\", \"Tiruvannamalai Stop\", \"Krishnagiri Stop\",\n",
    "    \"Namakkal Junction\", \"Virudhunagar Stop\", \"Perambalur Stop\", \"Ariyalur Stop\",\n",
    "    \"Dharmapuri Stop\", \"Tiruvarur Stop\", \"Ramanathapuram Stop\"\n",
    "]\n",
    "\n",
    "complaint_category_map = {\n",
    "    \"Garbage not cleared for 3 days\": \"Garbage\",\n",
    "    \"Overflowing garbage bins\": \"Garbage\",\n",
    "    \"Streetlight not working in main road\": \"Streetlight\",\n",
    "    \"Pothole near bus stop causing accidents\": \"Pothole\",\n",
    "    \"Water leakage from underground pipe\": \"Water Leakage\",\n",
    "    \"Bus delay causing inconvenience\": \"Transport Delay\",\n",
    "    \"Metro late arrival issue\": \"Transport Delay\",\n",
    "    \"Open drainage complaint\": \"Blocked Drain\",\n",
    "    \"Blocked stormwater drain\": \"Blocked Drain\",\n",
    "    \"Traffic signal not functioning\": \"Traffic Signal Issue\",\n",
    "    \"Illegal parking complaint\": \"Illegal Parking\",\n",
    "    \"No water supply issue\": \"Water Supply Issue\",\n",
    "    \"Road surface broken causing accidents\": \"Road Damage\",\n",
    "    \"Public toilet maintenance issue\": \"Public Toilet Issue\",\n",
    "    \"Street cleaning not done\": \"Street Cleaning Issue\",\n",
    "    \"Noise pollution complaint\": \"Noise Complaint\",\n",
    "    \"Encroachment complaint\": \"Encroachment\",\n",
    "    \"Unauthorized billboard issue\": \"Unauthorized Billboard\",\n",
    "    \"Air pollution from industries\": \"Air Pollution\"\n",
    "}\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Generate Neighborhoods\n",
    "# ------------------------------\n",
    "neigh_ids = [str(uuid.uuid4()) for _ in range(n)]\n",
    "neigh_names = [f\"Neighborhood-{i}\" for i in range(1, n+1)]\n",
    "latitudes = np.random.uniform(8.0, 13.5, n)\n",
    "longitudes = np.random.uniform(76.5, 80.5, n)\n",
    "\n",
    "start_date = datetime.now() - timedelta(days=365)\n",
    "\n",
    "# --------------------------\n",
    "# 1. 311 Service Request Data\n",
    "# --------------------------\n",
    "service_dates = [start_date + timedelta(minutes=np.random.randint(0, 525600)) for _ in range(n)]\n",
    "selected_complaints = np.random.choice(list(complaint_category_map.keys()), n)  # select complaints\n",
    "\n",
    "service_df = pd.DataFrame({\n",
    "    \"Neighborhood_ID\": neigh_ids,\n",
    "    \"Request_ID\": [str(uuid.uuid4()) for _ in range(n)],\n",
    "    \"DateTime\": [d.strftime(\"%d-%m-%Y %H:%M:%S\") for d in service_dates],\n",
    "    \"Complaints\": selected_complaints,\n",
    "    \"Category\": [complaint_category_map[c] for c in selected_complaints],  # map complaint → category\n",
    "    \"Location\": np.random.choice(tn_locations, n),\n",
    "    \"Stop_Name\": np.random.choice(tn_stops, n),\n",
    "    \"Latitude\": latitudes,\n",
    "    \"Longitude\": longitudes,\n",
    "    \"Status\": np.random.choice([\"Open\", \"Closed\"], n),\n",
    "    \"Resolution_Time(hrs)\": np.random.randint(1, 200, n)\n",
    "})\n",
    "service_df.to_csv(os.path.join(output_dir, \"311_service_requests.csv\"), index=False)\n",
    "\n",
    "print(\"✅ 311 Service Requests generated with complaint-related categories\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2. Public Transport Usage Data\n",
    "# --------------------------\n",
    "transport_dates = [start_date + timedelta(minutes=np.random.randint(0, 525600)) for _ in range(n)]\n",
    "scheduled_times = pd.to_datetime(transport_dates)\n",
    "actual_times = scheduled_times + pd.to_timedelta(np.random.randint(0, 60, n), unit='m')\n",
    "transport_df = pd.DataFrame({\n",
    "    \"Neighborhood_ID\": neigh_ids,\n",
    "    \"Route_ID\": [f\"R{str(i).zfill(3)}\" for i in range(n)],\n",
    "    \"Stop_Name\": np.random.choice(tn_stops, n),\n",
    "    \"Scheduled_Time\": scheduled_times.strftime(\"%d-%m-%Y %H:%M:%S\"),\n",
    "    \"Actual_Time\": actual_times.strftime(\"%d-%m-%Y %H:%M:%S\"),\n",
    "    \"Delay(mins)\": (actual_times - scheduled_times).seconds // 60,\n",
    "    \"Daily_Ridership\": np.random.randint(500, 5000, n)\n",
    "})\n",
    "transport_df.to_csv(os.path.join(output_dir, \"public_transport_usage.csv\"), index=False)\n",
    "\n",
    "# --------------------------\n",
    "# 3. Social Media Sentiment Data\n",
    "# --------------------------\n",
    "social_dates = [start_date + timedelta(minutes=np.random.randint(0, 525600)) for _ in range(n)]\n",
    "social_df = pd.DataFrame({\n",
    "    \"Neighborhood_ID\": neigh_ids,\n",
    "    \"Post_ID\": [str(uuid.uuid4()) for _ in range(n)],\n",
    "    \"User_Location\": np.random.choice(tn_locations, n),\n",
    "    \"Timestamp\": [d.strftime(\"%d-%m-%Y %H:%M:%S\") for d in social_dates],\n",
    "    # \"Text_Content\": np.random.choice(complaint_category_map, n),\n",
    "    \"Text_Content\": np.random.choice(list(complaint_category_map.keys()), n),\n",
    "    \"Sentiment_Score\": np.random.choice([\"Positive\", \"Negative\", \"Neutral\"], n)\n",
    "})\n",
    "social_df.to_csv(os.path.join(output_dir, \"social_media_sentiment.csv\"), index=False)\n",
    "\n",
    "# --------------------------\n",
    "# 4. Demographic Data\n",
    "# --------------------------\n",
    "demo_df = pd.DataFrame({\n",
    "    \"Neighborhood_ID\": neigh_ids,\n",
    "    \"Total_Population\": np.random.randint(1000, 100000, n),\n",
    "    \"Population_Density\": np.random.randint(5000, 20000, n),\n",
    "    \"Income_Level\": np.random.choice([\"Low\", \"Medium\", \"High\"], n),\n",
    "    \"Age_Group\": np.random.choice([\"0-14\", \"15-24\", \"25-44\", \"45-64\", \"65+\"], n),\n",
    "    \"Ward_ID\": [f\"Ward-{i}\" for i in range(1, n+1)],\n",
    "    \"Zone_ID\": np.random.choice([\"North\", \"South\", \"East\", \"West\", \"Central\"], n)\n",
    "})\n",
    "demo_df.to_csv(os.path.join(output_dir, \"demographics.csv\"), index=False)\n",
    "\n",
    "# --------------------------\n",
    "# 5. Neighborhood Shapefile\n",
    "# --------------------------\n",
    "# neigh_info_df = pd.DataFrame({\n",
    "#     \"Neighborhood_ID\": neigh_ids,\n",
    "#     \"Neighborhood_Name\": neigh_names,\n",
    "#     \"Latitude\": latitudes,\n",
    "#     \"Longitude\": longitudes\n",
    "# })\n",
    "# gdf = gpd.GeoDataFrame(neigh_info_df, geometry=gpd.points_from_xy(neigh_info_df.Longitude, neigh_info_df.Latitude))\n",
    "# gdf.set_crs(epsg=4326, inplace=True)\n",
    "# gdf.to_file(os.path.join(output_dir, \"neighborhoods.shp\"))\n",
    "\n",
    "# --------------------------\n",
    "# 6. Real-Time TN Public Complaints (Expanded version)\n",
    "# --------------------------\n",
    "# timestamps = [start_date + timedelta(minutes=np.random.randint(0, 525600)) for _ in range(n)]\n",
    "# status_options = [\"Open\", \"Closed\"]\n",
    "# resolution_times = np.random.randint(1, 200, n)\n",
    "\n",
    "# real_time_df = pd.DataFrame({\n",
    "#     \"Neighborhood_ID\": neigh_ids,\n",
    "#     \"Request_ID\": [str(uuid.uuid4()) for _ in range(n)],\n",
    "#     \"DateTime\": [ts.strftime(\"%d-%m-%Y %H:%M:%S\") for ts in timestamps],\n",
    "#     \"Category\": np.random.choice([\"Garbage\", \"Pothole\", \"Streetlight\", \"Water Leakage\"], n),\n",
    "#     \"Complaints\": np.random.choice(complaint_category_map, n),\n",
    "#     \"Location\": np.random.choice(tn_locations, n),\n",
    "#     \"Stop_Name\": np.random.choice(tn_stops, n),\n",
    "#     \"Latitude\": latitudes,\n",
    "#     \"Longitude\": longitudes,\n",
    "#     \"Status\": np.random.choice(status_options, n),\n",
    "#     \"Resolution_Time(hrs)\": resolution_times\n",
    "# })\n",
    "# real_time_df.to_csv(os.path.join(output_dir, \"real_time_tn_public_complaints.csv\"), index=False)\n",
    "\n",
    "# --------------------------\n",
    "# Optional: ZIP all datasets\n",
    "# --------------------------\n",
    "zip_path = os.path.join(output_dir, \"city_datasets_10000_realistic.zip\")\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\") or file.endswith((\".shp\", \".shx\", \".dbf\", \".prj\")):\n",
    "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), output_dir))\n",
    "\n",
    "print(f\"✅ All datasets saved independently & compressed into: {zip_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
